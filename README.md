# Visual Question Answering for Autonomous Driving

This project explores advancements in Visual Question Answering (VQA) within the realm of autonomous driving by synergizing Vision Transformers (ViT) and Large Language Models (LLMs). Our approach integrates ViT for in-depth visual feature extraction and LLMs for nuanced textual understanding, fostering a holistic comprehension of driving scenarios.

## Abstract

In this project, we aim to significantly improve accuracy in answering a broad spectrum of questions related to traffic signs, road conditions, and driver actions through pretraining on diverse multimodal datasets and subsequent fine-tuning for task-specific adaptation. This endeavor contributes to the evolving integration of computer vision and natural language processing, addressing the intricacies of real-world challenges in autonomous systems for enhanced interpretability and performance.

## Data

The data used in this project includes the NuScenes dataset, which is a multimodal dataset designed for autonomous driving applications. It consists of numerous labeled images and associated questions about urban driving scenarios.

## Models
### BERT_ViT: 
This model combines BERT (Bidirectional Encoder Representations from Transformers) with Vision Transformer (ViT) for effective understanding and interaction in a multimodal context.
### RoBERTa_ViT: 
An extension of BERT_ViT with RoBERTa (Robustly Optimized BERT Pretraining Approach) replacing BERT for improved performance on text processing.
## Results
Our models demonstrate the capability to understand complex visual scenes and provide accurate answers to related questions. Detailed performance metrics and evaluations are presented in the results section.
